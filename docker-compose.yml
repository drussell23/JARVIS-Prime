# ==============================================================================
# JARVIS-Prime Docker Compose Configuration
# ==============================================================================
# Orchestrates the Tier-0 Brain services including:
# - JARVIS-Prime API server (OpenAI-compatible)
# - llama.cpp inference backend
# - Model management and hot-swap support
#
# Usage:
#   docker-compose up -d                    # Start all services
#   docker-compose logs -f jarvis-prime     # View logs
#   docker-compose exec jarvis-prime python -m jarvis_prime.docker.model_manager download  # Download model
# ==============================================================================

version: "3.9"

services:
  # ============================================================================
  # JARVIS-Prime API Server
  # ============================================================================
  jarvis-prime:
    build:
      context: .
      dockerfile: Dockerfile
    image: jarvis-prime:latest
    container_name: jarvis-prime
    restart: unless-stopped

    # Resource limits (optimized for 16GB MacBook)
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 10G
        reservations:
          cpus: "2"
          memory: 4G

    # Environment configuration
    environment:
      - JARVIS_PRIME_HOST=0.0.0.0
      - JARVIS_PRIME_PORT=8000
      - MODEL_PATH=/app/models/current.gguf
      - TELEMETRY_DIR=/app/telemetry
      - LLAMA_SERVER_HOST=127.0.0.1
      - LLAMA_SERVER_PORT=8080
      - LLAMA_N_CTX=${LLAMA_N_CTX:-4096}
      - LLAMA_N_BATCH=${LLAMA_N_BATCH:-512}
      - LLAMA_N_GPU_LAYERS=${LLAMA_N_GPU_LAYERS:-0}
      - LLAMA_THREADS=${LLAMA_THREADS:-4}
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
      # Hot-swap configuration
      - HOT_SWAP_ENABLED=true
      - MODEL_WATCH_DIR=/app/models
      # Reactor-core integration
      - REACTOR_CORE_WATCH_DIR=/app/reactor-core-output

    # Port mappings
    ports:
      - "${JARVIS_PRIME_PORT:-8000}:8000"

    # Volume mounts
    volumes:
      # Models directory - persistent storage for GGUF models
      - ./models:/app/models
      # Telemetry output for reactor-core training
      - ./telemetry:/app/telemetry
      # Logs
      - ./logs:/app/logs
      # Reactor-core output directory (for receiving trained models)
      - ${REACTOR_CORE_OUTPUT:-./reactor-core-output}:/app/reactor-core-output:ro

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"

    # Network
    networks:
      - jarvis-network

  # ============================================================================
  # Model Downloader (One-shot utility)
  # ============================================================================
  model-downloader:
    build:
      context: .
      dockerfile: Dockerfile
    image: jarvis-prime:latest
    container_name: jarvis-prime-downloader
    profiles:
      - tools  # Only runs when explicitly invoked

    environment:
      - HF_TOKEN=${HF_TOKEN:-}
      - MODEL_REPO=${MODEL_REPO:-TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF}
      - MODEL_FILE=${MODEL_FILE:-tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf}
      - MODEL_OUTPUT=/app/models/current.gguf

    volumes:
      - ./models:/app/models

    command: ["download", "--repo", "${MODEL_REPO:-TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF}", "--file", "${MODEL_FILE:-tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf}"]

    networks:
      - jarvis-network

  # ============================================================================
  # Model Converter (GGUF conversion utility)
  # ============================================================================
  model-converter:
    build:
      context: .
      dockerfile: Dockerfile
    image: jarvis-prime:latest
    container_name: jarvis-prime-converter
    profiles:
      - tools  # Only runs when explicitly invoked

    environment:
      - QUANTIZE_METHOD=${QUANTIZE_METHOD:-q4_k_m}

    volumes:
      - ./models:/app/models
      - ${HF_MODEL_PATH:-./hf-models}:/app/hf-models:ro

    command: ["convert", "--input", "/app/hf-models", "--output", "/app/models/converted.gguf", "--quantize", "${QUANTIZE_METHOD:-q4_k_m}"]

    networks:
      - jarvis-network

# ==============================================================================
# Networks
# ==============================================================================
networks:
  jarvis-network:
    driver: bridge
    name: jarvis-prime-network

# ==============================================================================
# Volumes (named volumes for persistence)
# ==============================================================================
volumes:
  models:
    name: jarvis-prime-models
  telemetry:
    name: jarvis-prime-telemetry
  logs:
    name: jarvis-prime-logs
